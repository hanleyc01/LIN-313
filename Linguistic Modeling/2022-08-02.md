# Language Models Continued, GPT-3
- Language models are basically a way of representing and predicting the most likely segment of speech to follow a sequence of words, characters, sounds, etc.
	- "It was the best of times, it was the worst of `blank`" -> we have some intuitions about what ought to follow, but generally, aside from familiarity with the quote, we know that the word to follow must be able to be described as "the worst of"
- n-gram language models:
	- When dealing with words, n-gram models are a way of predicting the next word based on the n-1 preceding words; this is used to model probabilities
		- Unigram: P(word_i | context) = P(word_i)
		- Bigram: P(word_i | cxt) = P(word_i | word_i-1)
		- Trigram: P(word_i | cxt) = P(word_i | word_i-1, word_i-2)
		- n-gram: P(word_i | cxt) = P(word_i | word_i-1, ..., word_i-n )
	- Consider the following sentence: "The cat gave the dog the fig."
		- Unigram:
			- P("The") = # "the" in corpus / length(corpus) = 3/7
			- P("dog") = 1/7
		- Bigram:
			- P("cat" | cxt) = P("cat" | "the") = # occurences of "cat" following "the" / # of occurences of "the" = 1 / 3
	- Bigram model is the way of using the previous word for determining what candidate is the most probable.
- Markov model provides us with a state machine to model the way a sentence is formed; this we can account for the state transitions within the automaton using probability
	- We can use this Markov model to generate texts